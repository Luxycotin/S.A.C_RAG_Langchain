{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación Práctica y Exploración de RAG con Langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementacion Práctica de RAG\n",
    "\n",
    "En esta seccion se implementa desde cero un sistema RAG usando Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración base y utilidades\n",
    "# pip install langchain langchain_community langchain-text-splitters langchain-ollama langchain-chroma langchain-google-genai chromadb pandas\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_DIR = Path().resolve()\n",
    "DATA_FILE = PROJECT_DIR / \"El coleccionista de sonidos.txt\"\n",
    "DEFAULT_SPLIT = {\"chunk_size\": 320, \"chunk_overlap\": 40}\n",
    "\n",
    "API_KEY = os.getenv(\"API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    print(\"Aviso: define API_KEY o GOOGLE_API_KEY para usar Gemini; también puedes probar solo con Ollama.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga y preparación de un documento de ejemplo\n",
    "\n",
    "Para efectos de prueba, creamos un archivo textual con datos random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relato_base = \"\"\"\n",
    "Clara nació en un faro que ya no guía barcos, pero sí recoge historias. Su pasatiempo favorito consiste en grabar murmullos del pueblo costero y clasificarlos como si fueran minerales.\n",
    "Conserva en frascos el zumbido de las redes mojadas, el crujido del pan cuando sale del horno y los latidos apurados de quienes anuncian tormenta.\n",
    "Un jueves de niebla quiso atrapar el silencio que antecede a una confesión y descubrió que no cabe en ningún recipiente; apenas roza el vidrio y se transforma en luz.\n",
    "Desde ese día comenzó a coleccionar silencios en un cuaderno: anota dónde aparecieron, qué olor tenían y qué conversación salvaron.\n",
    "No dejó de escuchar sonidos, pero aprendió que algunos momentos se entienden mejor cuando se mezclan ambos.\n",
    "\"\"\"\n",
    "\n",
    "with open(DATA_FILE, \"w\", encoding=\"utf-8\") as archivo_destino:\n",
    "    archivo_destino.write(relato_base.strip())\n",
    "\n",
    "print(f\"Archivo de ejemplo actualizado en {DATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline RAG: Carga, Split, Embedding, Indexado y QA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def preparar_fragmentos(path=DATA_FILE, split_config=DEFAULT_SPLIT):\n",
    "    loader = TextLoader(str(path))\n",
    "    documentos = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=split_config[\"chunk_size\"],\n",
    "        chunk_overlap=split_config[\"chunk_overlap\"],\n",
    "    )\n",
    "    return documentos, splitter.split_documents(documentos)\n",
    "\n",
    "def inicializar_llm(api_key):\n",
    "    try:\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API_KEY no configurada\")\n",
    "        return ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=api_key)\n",
    "    except Exception as exc:\n",
    "        print(f\"No se pudo iniciar Gemini: {exc}. Usa ChatOllama o cualquier otro LLM disponible.\")\n",
    "        return None\n",
    "\n",
    "EMBED_MODEL_NAME = \"nomic-embed-text\"\n",
    "docs, chunks = preparar_fragmentos()\n",
    "embedding_model = OllamaEmbeddings(model=EMBED_MODEL_NAME)\n",
    "db = Chroma.from_documents(chunks, embedding_model)\n",
    "llm = inicializar_llm(API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Función de pregunta-respuesta sobre el contenido usando RAG**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consultar_relato(pregunta, store=db, modelo=llm, k=3):\n",
    "    if store is None:\n",
    "        raise ValueError(\"La base vectorial no fue inicializada\")\n",
    "    docs = store.similarity_search(pregunta, k=k)\n",
    "    contexto = \"\\n---\\n\".join(d.page_content for d in docs)\n",
    "    prompt = (\n",
    "        \"Responde solo con el siguiente contexto.\\n\"\n",
    "        f\"Contexto:\\n{contexto}\\n\\nPregunta: {pregunta}\\nRespuesta:\"\n",
    "    )\n",
    "    if modelo is None:\n",
    "        raise RuntimeError(\"No hay LLM activo. Configura Gemini u Ollama antes de consultar.\")\n",
    "    respuesta = modelo.invoke(prompt)\n",
    "    return respuesta.content if hasattr(respuesta, \"content\") else respuesta\n",
    "\n",
    "pregunta_ejemplo = \"¿Qué empezó a coleccionar Clara además de sonidos?\"\n",
    "print(consultar_relato(pregunta_ejemplo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. uso de Modelos de Embedding y LLM\n",
    "\n",
    "Se prueban distintos modelos de embeddings y LLM compatibles para observar como recupera documentos y como genera las respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_setups = [\n",
    "    (\"nomic-embed-text\", OllamaEmbeddings(model=\"nomic-embed-text\")),\n",
    "    (\"embeddinggemma\", OllamaEmbeddings(model=\"embeddinggemma\")),\n",
    "]\n",
    "\n",
    "query_text = \"memorias guardadas en frascos\"\n",
    "\n",
    "for name, emb in embedding_setups:\n",
    "    try:\n",
    "        db_temp = Chroma.from_documents(chunks, emb, persist_directory=f\"./chroma_{name}\")\n",
    "        docs_found = db_temp.similarity_search(query_text, k=2)\n",
    "        print(f\"\\nEmbedding: {name}\")\n",
    "        for d in docs_found:\n",
    "            print(f\"> Fragmento: {d.page_content[:90]}...\")\n",
    "    except Exception as exc:\n",
    "        print(f\"Error con embedding {name}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def benchmark_embeddings(consulta, configuraciones):\n",
    "    resultados = []\n",
    "    for nombre, emb in configuraciones:\n",
    "        try:\n",
    "            start = time.time()\n",
    "            db_temp = Chroma.from_documents(chunks, emb, persist_directory=f\"./benchmark_{nombre}\")\n",
    "            db_temp.similarity_search(consulta, k=2)\n",
    "            elapsed = round(time.time() - start, 3)\n",
    "            resultados.append({\"Modelo\": nombre, \"Tiempo (seg)\": elapsed})\n",
    "            print(f\"Embedding {nombre} listo en {elapsed} seg\")\n",
    "        except Exception as exc:\n",
    "            resultados.append({\"Modelo\": nombre, \"Tiempo (seg)\": \"Error\", \"Detalle\": str(exc)})\n",
    "            print(f\"Fallo {nombre}: {exc}\")\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "benchmark_embeddings(\"inteligencia artificial experta en relatos\", embedding_setups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain_ollama import ChatOllama\n",
    "\n",
    "    llm_disponibles = [\n",
    "        (\"Gemini 2.0 Flash\", llm),\n",
    "        (\"qwen2.5:0.5b\", ChatOllama(model=\"qwen2.5:0.5b\")),\n",
    "    ]\n",
    "\n",
    "    for nombre, modelo_llm in llm_disponibles:\n",
    "        if modelo_llm is None:\n",
    "            print(f\"Modelo {nombre} no inicializado. Se omite.\")\n",
    "            continue\n",
    "        try:\n",
    "            print(f\"\n",
    ">>> {nombre}\")\n",
    "            respuesta = consultar_relato(\"¿De qué trata el relato completo?\", store=db, modelo=modelo_llm)\n",
    "            print(respuesta)\n",
    "        except Exception as exc:\n",
    "            print(f\"Error usando {nombre}: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Optimizacion del Separador de Texto\n",
    "\n",
    "Se prueba variación en los parametros `chunk_size` y `chunk_overlap` del CharacterTextSplitter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    sizes = [\n",
    "        (400, 80),\n",
    "        (700, 120),\n",
    "    ]\n",
    "\n",
    "    for cs, co in sizes:\n",
    "        splitter = CharacterTextSplitter(chunk_size=cs, chunk_overlap=co)\n",
    "        pedazos = splitter.split_documents(docs)\n",
    "        print(f\"chunk_size={cs}, chunk_overlap={co} => chunks: {len(pedazos)}\")\n",
    "        print(f\"> Ejemplo de chunk: {pedazos[0].page_content[:60]}...\n",
    "\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Gestionar Bases de Datos Vectoriales\n",
    "\n",
    "Esta seccion es para la persistencia, colecciones y búsquedas avanzadas con Chroma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"./chroma_db\"\n",
    "db_persist = Chroma.from_documents(chunks, embedding_model, persist_directory=persist_path)\n",
    "\n",
    "print(f\"Base de datos vectorial guardada en {persist_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in chunks:\n",
    "    d.metadata[\"origen\"] = DATA_FILE.name\n",
    "\n",
    "db_filtrada = Chroma.from_documents(chunks, embedding_model)\n",
    "results = db_filtrada.similarity_search_with_score(\n",
    "    \"frascos\",\n",
    "    k=5,\n",
    "    filter={\"origen\": DATA_FILE.name},\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(f\"Score: {score:.3f} | Fragmento: {doc.page_content[:70]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Refinamiento de Prompts\n",
    "\n",
    "Experimentamos con diferentes plantillas y técnicas de prompt engineering para mejorar la calidad, relevancia y trazabilidad de las respuestas generadas por el LLM dentro del sistema RAG.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "    base_prompt = \"\"\"Eres un analista literario y solo puedes usar el bloque {context} para responder.\n",
    "    Señala la evidencia con comillas si es útil. Debes decir \"No lo sé\" si el texto no contiene la respuesta.\n",
    "\n",
    "    Texto:\n",
    "    {context}\n",
    "\n",
    "    Pregunta:\n",
    "    {question}\n",
    "\n",
    "    Conclusión fundamentada:\"\"\"\n",
    "    plantilla = PromptTemplate(template=base_prompt, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    def consultar_con_plantilla(pregunta, store=db, modelo=llm, k=4, prompt_template=plantilla):\n",
    "        docs = store.similarity_search(pregunta, k=k)\n",
    "        contexto = \"\n",
    "\n",
    "\".join(d.page_content for d in docs)\n",
    "        prompt = prompt_template.format(context=contexto, question=pregunta)\n",
    "        respuesta = modelo.invoke(prompt)\n",
    "        return respuesta.content if hasattr(respuesta, 'content') else respuesta\n",
    "\n",
    "    pregunta_2 = \"¿Quién es Clara?\"\n",
    "    print(consultar_con_plantilla(pregunta_2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_langchain_ollama_tp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}